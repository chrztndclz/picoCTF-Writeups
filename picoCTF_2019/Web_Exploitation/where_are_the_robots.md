## where are the robots

**Platform:** picoCTF 2019

**Author:** ZARATEC/DANNY

**Category:** Web Exploitation

**Difficulty:** Easy

Tags:
#picoCTF_2019 
#Web_Exploitation 
#Easy 

---

## Description
Can you find the robots? https://jupiter.challenges.picoctf.org/problem/56830/ (link) or http://jupiter.challenges.picoctf.org:56830


---

## Hints
- What part of the website could tell you where the creator doesn't want you to look?

---

## Analysis

The challenge involves inspecting the website for hidden resources. In most web challenges, the `robots.txt` file contains directories or files the creator "hides" from normal visitors. These hidden paths can often contain the **flag**.


---

## Methodology
**Step 1: Open the terminal**  
We will use terminal commands to interact with the website directly instead of a browser.

**Step 2: Access the `robots.txt` file**  
The `robots.txt` file is usually located at the root of the website. Use the following `curl` command:

`curl https://jupiter.challenges.picoctf.org:56830/robots.txt`

> **Explanation:** [curl](https://curl.se/) is a command-line tool to fetch content from a URL. It allows you to view web pages or files without opening a browser.

**Step 3: Inspect the content of `robots.txt`**

The file contains rules like:

`User-agent: * Disallow: /1bb4c.html`

Here, `/1bb4c.html` is a path that the website owner is signaling bots not to visit. In CTFs, this usually points to a hidden file or page containing the flag.


<img width="589" height="77" alt="image" src="https://github.com/user-attachments/assets/e0c3328c-1048-4a47-96f1-5b95ea8e83f8" />


**Step 4: Access the hidden file and extract the flag**

Replace `robots.txt` with the disallowed path:


<img width="741" height="45" alt="image" src="https://github.com/user-attachments/assets/c71f525e-b680-45df-bf91-d2e2712c4ad1" />


- `-s` makes `curl` silent (hides progress info).
    
- `grep "picoCTF"` filters the output to only show the flag.
    

**Step 5: Retrieve the flag**


---

## Flag

picoCTF{ca1----------b4c}

---

## Reflection
This challenge highlights how `robots.txt` can inadvertently leak sensitive paths. While it’s meant for search engines, in CTFs it’s commonly used as a hint for hidden files. Understanding how to read and follow these hints is a critical skill in web exploitation challenges.

---

## Tools

- curl – fetch content from a website via command line
    
- grep – search for patterns in text
    

---

## References

- [Robots.txt standard](https://www.robotstxt.org/)
    
- [Using curl](https://curl.se/docs/manpage.html)
    
- [Using grep](https://www.gnu.org/software/grep/manual/grep.html)

